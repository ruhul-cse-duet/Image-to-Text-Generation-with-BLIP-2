# ЁЯЪА T5Gemma Complete Setup Guide (ржмрж╛ржВрж▓рж╛ржпрж╝)

## тЬЕ ржХрж┐ ржкрж░рж┐ржмрж░рзНрждржи рж╣ржпрж╝рзЗржЫрзЗ

ржЖржкржирж╛рж░ project ржП ржПржЦржи **Google T5Gemma-2-1B** model setup ржХрж░рж╛ рж╣ржпрж╝рзЗржЫрзЗ ржпрж╛ Kaggle ржП ржЖржкржирж┐ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗржЫрж┐рж▓рзЗржиред

### ЁЯОп T5Gemma ржПрж░ ржмрж┐рж╢рзЗрж╖рждрзНржм:

1. **ржжрзНрж░рзБрждрждрж░**: 1B parameters (BLIP-2 ржПрж░ 2.7B ржерзЗржХрзЗ ржЫрзЛржЯ)
2. **ржХржо Memory**: ржорж╛рждрзНрж░ 2GB VRAM рж▓рж╛ржЧрзЗ
3. **Kaggle Compatible**: Kaggle ржП ржпрзЗржнрж╛ржмрзЗ ржХрж╛ржЬ ржХрж░ржЫрж┐рж▓ рж╕рзЗржнрж╛ржмрзЗржЗ
4. **рж╕ржарж┐ржХ Format**: `<start_of_image>` token рж╕рж╣

---

## ЁЯУЭ ржкрж░рж┐ржмрж░рзНрждрж┐ржд ржлрж╛ржЗрж▓рж╕ржорзВрж╣

### 1. `huggingface_model/loader.py` тЬЕ
```python
# Model changed to T5Gemma
model_name = "google/t5gemma-2-1b-1b"

# AutoModelForCausalLM ржмрзНржпржмрж╣рж╛рж░
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    trust_remote_code=True  # Important!
)
```

### 2. `routes/main.py` тЬЕ
```python
# Prompt format: "<start_of_image> your text"
formatted_prompt = f"<start_of_image> {prompt}"

# Processing
inputs = processor(
    text=formatted_prompt,
    images=image,
    return_tensors="pt"
)
```

### 3. `test_model.py` тЬЕ
- T5Gemma specific testing
- Correct prompt format verification

### 4. `README.md` тЬЕ
- Complete T5Gemma documentation
- Performance benchmarks
- Best practices

---

## ЁЯФз Installation Steps

### ржзрж╛ржк рзз: Environment Activate ржХрж░рзБржи

```bash
# Anaconda environment activate ржХрж░рзБржи
C:\Users\Administrator\anaconda3\envs\Image-to-text_generation_with_BLIP-2\Scripts\activate
```

### ржзрж╛ржк рзи: Dependencies Check ржХрж░рзБржи

```bash
# Check Python version
python --version  # Should be 3.8+

# Check installed packages
pip list | findstr transformers
pip list | findstr torch
```

### ржзрж╛ржк рзй: ржпржжрж┐ ржжрж░ржХрж╛рж░ рж╣ржпрж╝ рждрж╛рж╣рж▓рзЗ Update ржХрж░рзБржи

```bash
# Update transformers
pip install --upgrade transformers

# Update torch (if needed)
pip install --upgrade torch torchvision

# Install accelerate
pip install --upgrade accelerate
```

### ржзрж╛ржк рзк: Model Test ржХрж░рзБржи

```bash
# Model load рж╣ржЪрзНржЫрзЗ ржХрж┐ржирж╛ test ржХрж░рзБржи
python test_model.py
```

**ржкрзНрж░ржержоржмрж╛рж░ run ржХрж░рж▓рзЗ:**
- Model download рж╢рзБрж░рзБ рж╣ржмрзЗ (~3.5GB)
- 5-15 ржорж┐ржирж┐ржЯ рж╕ржоржпрж╝ рж▓рж╛ржЧржмрзЗ
- Cache location: `C:\Users\Administrator\.cache\huggingface\`

### ржзрж╛ржк рзл: Application Run ржХрж░рзБржи

```bash
python app.py
```

Browser ржП: `http://127.0.0.1:5000`

---

## ЁЯОп T5Gemma ржмрзНржпржмрж╣рж╛рж░рзЗрж░ ржирж┐ржпрж╝ржо

### тЬЕ рж╕ржарж┐ржХ Prompt Format:

T5Gemma ржПржХржЯрж┐ text completion modelред ржПржЯрж┐ incomplete sentences complete ржХрж░рзЗред

**Good Prompts:**
```
"in this image, there is"
"describe this image:"
"the main object is"
"this picture shows"
"what I see here is"
```

**Bad Prompts:**
```
"describe"  # Too short
"tell me everything about this image in detail"  # Too long
```

### ЁЯУЛ Kaggle ржПрж░ ржорждрзЛ ржмрзНржпржмрж╣рж╛рж░:

Kaggle ржП ржЖржкржирж┐ ржПржнрж╛ржмрзЗ ржХрж░рзЗржЫрж┐рж▓рзЗржи:
```python
generator(
    "image_url",
    text="<start_of_image> in this image, there is",
    generate_kwargs={"do_sample": False, "max_new_tokens": 50}
)
```

ржЖржкржирж╛рж░ application ржП:
1. Image upload ржХрж░рзБржи
2. Prompt рж▓рж┐ржЦрзБржи: "in this image, there is"
3. Application automatically `<start_of_image>` token add ржХрж░ржмрзЗ
4. Generate click ржХрж░рзБржи

---

## ЁЯФН Error Fix ржХрж░рж╛ рж╣ржпрж╝рзЗржЫрзЗ

### тЭМ ржЖржЧрзЗрж░ Error:
```
ValueError: Prompt contained 0 image tokens but received 1 images
```

### тЬЕ ржХрзЗржи рж╣ржпрж╝рзЗржЫрж┐рж▓:
- PaliGemma `<image>` token ржЪрж╛ржЗржЫрж┐рж▓
- T5Gemma `<start_of_image>` token ржЪрж╛ржпрж╝
- ржЖржорж░рж╛ token ржжрж┐ржЪрзНржЫрж┐рж▓рж╛ржо ржирж╛

### тЬЕ Solution:
```python
# ржПржЦржи automatically add рж╣ржпрж╝
formatted_prompt = f"<start_of_image> {prompt}"
```

---

## ЁЯУК T5Gemma ржПрж░ рж╕рзБржмрж┐ржзрж╛

| Feature | Value |
|---------|-------|
| **Model Size** | ~3.5GB (ржЫрзЛржЯ!) |
| **Parameters** | 1B (fast!) |
| **Speed** | 1-3 seconds (GPU) |
| **Memory** | 2GB VRAM |
| **Accuracy** | Good for quick tasks |
| **Cost** | Free (Hugging Face) |

### ржЕржирзНржпрж╛ржирзНржп Models ржПрж░ рж╕рж╛ржерзЗ рждрзБрж▓ржирж╛:

```
T5Gemma-1B:    тЪбтЪбтЪб (ржжрзНрж░рзБрждрждржо)
BLIP-2:        тЪбтЪб   (ржорж╛ржЭрж╛рж░рж┐)
PaliGemma-3B:  тЪб     (ржзрзАрж░ ржХрж┐ржирзНрждрзБ accurate)
```

---

## ЁЯТб Usage Examples

### Example 1: Simple Description
```
Image: ржПржХржЯрж┐ cat ржПрж░ ржЫржмрж┐
Prompt: "in this image, there is"
Output: "a cat sitting on a table"
```

### Example 2: Detailed Scene
```
Image: ржПржХржЯрж┐ park ржПрж░ ржЫржмрж┐
Prompt: "describe this scene:"
Output: "a beautiful park with trees, benches, and people walking"
```

### Example 3: Object Detection
```
Image: ржПржХржЯрж┐ room ржПрж░ ржЫржмрж┐
Prompt: "what objects are visible?"
Output: "a sofa, table, lamp, and window"
```

---

## ЁЯЫая╕П Troubleshooting

### рж╕ржорж╕рзНржпрж╛ рзз: Model Load рж╣ржЪрзНржЫрзЗ ржирж╛

```bash
# Cache clear ржХрж░рзБржи
rmdir /s %USERPROFILE%\.cache\huggingface

# Dependencies reinstall ржХрж░рзБржи
pip install --upgrade transformers accelerate torch

# ржЖржмрж╛рж░ test ржХрж░рзБржи
python test_model.py
```

### рж╕ржорж╕рзНржпрж╛ рзи: "trust_remote_code" Error

```bash
# transformers update ржХрж░рзБржи
pip install transformers>=4.57.2
```

### рж╕ржорж╕рзНржпрж╛ рзй: CUDA Error

```bash
# CPU mode use ржХрж░рзБржи
# loader.py рждрзЗ change ржХрж░рзБржи:
device = "cpu"
```

### рж╕ржорж╕рзНржпрж╛ рзк: Slow Performance

**Solutions:**
1. GPU use ржХрж░рзБржи (if available)
2. Image size ржХржорж╛ржи (max 1024x1024)
3. `max_new_tokens=100` set ржХрж░рзБржи (routes/main.py рждрзЗ)

---

## ЁЯОУ Advanced Configuration

### Generation Parameters Tuning

`routes/main.py` рждрзЗ customize ржХрж░рзБржи:

```python
output_ids = model.generate(
    **inputs,
    max_new_tokens=200,      # ржЖрж░рзЛ рж▓ржорзНржмрж╛ output
    do_sample=True,          # creative output
    temperature=0.7,         # randomness
    top_p=0.9,              # diversity
    repetition_penalty=1.2   # avoid repetition
)
```

### Memory Optimization

`loader.py` рждрзЗ:

```python
# Use 8-bit quantization
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_8bit=True,  # Half memory!
    device_map="auto"
)
```

---

## тЬЕ Final Checklist

- [ ] Environment activated: `conda activate Image-to-text_generation_with_BLIP-2`
- [ ] Dependencies installed: `pip install -r requirements.txt`
- [ ] Model test successful: `python test_model.py`
- [ ] Application running: `python app.py`
- [ ] Browser accessible: `http://127.0.0.1:5000`
- [ ] Image upload works
- [ ] Text generation works
- [ ] Results look good

---

## ЁЯЪА Quick Start Commands

```bash
# 1. Activate environment
C:\Users\Administrator\anaconda3\envs\Image-to-text_generation_with_BLIP-2\Scripts\activate

# 2. Test model
python test_model.py

# 3. Run app
python app.py

# 4. Open browser
start http://127.0.0.1:5000
```

ржЕржержмрж╛ рж╕рж╣ржЬржнрж╛ржмрзЗ double-click: `start.bat`

---

## ЁЯОЙ Success!

ржпржжрж┐ рж╕ржм ржарж┐ржХржарж╛ржХ ржХрж╛ржЬ ржХрж░рзЗ, рждрж╛рж╣рж▓рзЗ ржЖржкржирж╛рж░ T5Gemma application ready! ЁЯОК

**Next Steps:**
1. тЬЕ Different images test ржХрж░рзБржи
2. тЬЕ Different prompts try ржХрж░рзБржи
3. тЬЕ Performance check ржХрж░рзБржи
4. ЁЯОи UI customize ржХрж░рзБржи (optional)
5. ЁЯЪА Deploy ржХрж░рзБржи (optional)

---

## ЁЯУЮ рж╕рж╛рж╣рж╛ржпрзНржп ржжрж░ржХрж╛рж░?

ржпржжрж┐ ржХрзЛржирзЛ рж╕ржорж╕рзНржпрж╛ рж╣ржпрж╝:
1. Complete error message copy ржХрж░рзБржи
2. `python test_model.py` output share ржХрж░рзБржи
3. System info ржжрж┐ржи (GPU/CPU, RAM, OS)
4. ржЬрж╛ржирж╛ржи, ржЖржорж┐ рж╕рж╛рж╣рж╛ржпрзНржп ржХрж░ржм! ЁЯдЭ

---

**рж▓рж┐ржЦрзЗржЫрзЗржи**: AI Assistant  
**рждрж╛рж░рж┐ржЦ**: December 27, 2025  
**Version**: 4.0.0 (T5Gemma Migration)  
**Model**: google/t5gemma-2-1b-1b

**Happy Coding! ЁЯЪАтЬи**
